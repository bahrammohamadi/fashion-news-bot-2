import os
import asyncio
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from telegram import Bot
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.exception import AppwriteException
from appwrite.query import Query

async def main(event=None, context=None):
    print("[INFO] Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ main Ø´Ø±ÙˆØ¹ Ø´Ø¯")

    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHANNEL_ID')
    gapgpt_key = os.environ.get('GAPGPT_API_KEY')
    appwrite_endpoint = os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1')
    appwrite_project = os.environ.get('APPWRITE_PROJECT_ID')
    appwrite_key = os.environ.get('APPWRITE_API_KEY')
    database_id = os.environ.get('APPWRITE_DATABASE_ID')
    collection_id = 'history'

    if not all([token, chat_id, gapgpt_key, appwrite_project, appwrite_key, database_id]):
        print("[ERROR] Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ù†Ø§Ù‚Øµ!")
        return {"status": "error"}

    bot = Bot(token=token)

    client = AsyncOpenAI(
        api_key=gapgpt_key,
        base_url="https://api.gapgpt.app/v1"
    )

    aw_client = Client()
    aw_client.set_endpoint(appwrite_endpoint)
    aw_client.set_project(appwrite_project)
    aw_client.set_key(appwrite_key)
    databases = Databases(aw_client)

    rss_feeds = [
        "https://medopia.ir/feed/",
        "https://www.digistyle.com/mag/feed/",
        "https://www.chibepoosham.com/feed/",
        "https://www.tarahanelebas.com/feed/",
        "https://www.persianpood.com/feed/",
        "https://www.jument.style/feed/",
        "https://www.zibamoon.com/feed/",
        "https://www.sarak-co.com/feed/",
        "https://www.elsana.com/feed/",
        "https://www.beytoote.com/rss/fashion",
        "https://www.namnak.com/rss/fashion",
        "https://www.modetstyle.com/feed/",
        "https://www.antikstyle.com/feed/",
        "https://www.rnsfashion.com/feed/",
        "https://www.pattonjameh.com/feed/",
        "https://www.tonikaco.com/feed/",
        "https://www.zoomit.ir/feed/category/fashion-beauty/",
        "https://www.khabaronline.ir/rss/category/Ù…Ø¯-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
        "https://fararu.com/rss/category/Ù…Ø¯-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
        "https://www.digikala.com/mag/feed/?category=Ù…Ø¯-Ùˆ-Ø²ÛŒØ¨Ø§ÛŒÛŒ",
    ]

    now = datetime.now(timezone.utc)
    time_threshold = now - timedelta(days=4)

    posted_count = 0
    max_posts_per_run = 1  # ÙÙ‚Ø· Û± Ù¾Ø³Øª Ø¯Ø± Ù‡Ø± Ø§Ø¬Ø±Ø§

    for feed_url in rss_feeds:
        if posted_count >= max_posts_per_run:
            break

        try:
            feed = feedparser.parse(feed_url)
            if not feed.entries:
                continue

            for entry in feed.entries:
                if posted_count >= max_posts_per_run:
                    break

                published = entry.get('published_parsed') or entry.get('updated_parsed')
                if not published:
                    continue
                pub_date = datetime(*published[:6], tzinfo=timezone.utc)
                if pub_date < time_threshold:
                    continue

                title = entry.title.strip()
                link = entry.link.strip()
                raw_html = entry.get('summary') or entry.get('description') or ''

                soup = BeautifulSoup(raw_html, 'html.parser')
                content_raw = soup.get_text(separator=' ').strip()

                # Ø¯Ø±Ø®ÙˆØ§Ø³Øª ÙˆØ§Ø­Ø¯ Ø¨Ù‡ GapGPT (ÙÛŒÙ„ØªØ± + ØªØ±Ø¬Ù…Ù‡ + Ù…Ù‚Ø§Ù„Ù‡ ÙØ´Ù†)
                final_content = await process_full_fashion_post(client, title, content_raw, link, pub_date, feed_url)
                if not final_content:
                    print(f"[SKIP] Ù¾Ø³Øª Ø±Ø¯ Ø´Ø¯: {title[:60]}")
                    continue

                final_text = f"{final_content}\n\nğŸ”— {link}"

                image_url = get_image_from_rss(entry)
                if not image_url:
                    image_url = await extract_image_from_page(link)

                try:
                    if image_url:
                        await bot.send_photo(chat_id=chat_id, photo=image_url, caption=final_text, parse_mode='HTML', disable_notification=True)
                    else:
                        await bot.send_message(chat_id=chat_id, text=final_text, disable_web_page_preview=True, disable_notification=True)

                    posted_count += 1
                    print(f"[SUCCESS] Ù¾Ø³Øª Ù…ÙˆÙÙ‚: {title[:60]}")

                    try:
                        databases.create_document(
                            database_id=database_id,
                            collection_id=collection_id,
                            document_id='unique()',
                            data={
                                'link': link,
                                'title': title,
                                'published_at': now.isoformat(),
                                'feed_url': feed_url
                            }
                        )
                    except Exception as save_err:
                        print(f"[WARN] Ø®Ø·Ø§ Ø°Ø®ÛŒØ±Ù‡ DB: {str(save_err)}")

                except Exception as send_err:
                    print(f"[ERROR] Ø®Ø·Ø§ Ø§Ø±Ø³Ø§Ù„: {str(send_err)}")

        except Exception as feed_err:
            print(f"[ERROR] Ø®Ø·Ø§ ÙÛŒØ¯ {feed_url}: {str(feed_err)}")

    print(f"[INFO] Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ - ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø³Øª Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡: {posted_count}")
    return {"status": "success", "posted": posted_count}


async def process_full_fashion_post(client, title, content_raw, link, pub_date, feed_url):
    prompt = f"""
Ø§ÙˆÙ„ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø¯Ø± Ø­ÙˆØ²Ù‡ Ù…Ø¯ØŒ ÙØ´Ù†ØŒ Ø§Ø³ØªØ§ÛŒÙ„ØŒ Ø²ÛŒØ¨Ø§ÛŒÛŒØŒ Ù„Ø¨Ø§Ø³ØŒ ØªØ±Ù†Ø¯ Ù¾ÙˆØ´Ø§Ú©ØŒ Ø·Ø±Ø§Ø­ÛŒ Ù„Ø¨Ø§Ø³ ÛŒØ§ Ø§Ø³ØªØ§ÛŒÙ„ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø³ØªØŸ Ø§Ú¯Ø± Ù†Ù‡ØŒ ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³ "ØºÛŒØ±Ù…Ø±ØªØ¨Ø·".

Ø§Ú¯Ø± Ø¨Ù„Ù‡ØŒ Ø§ÛŒÙ† Ú©Ø§Ø±Ù‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡:

Û±. ØªØ±Ø¬Ù…Ù‡ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù† Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø§Ù†ØªØ´Ø§Ø± Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ Ù…Ø¯ (Ø­ÙØ¸ Ù„Ø­Ù†ØŒ Ø³Ø§Ø®ØªØ§Ø± Ùˆ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª ØªØ®ØµØµÛŒ).

Û². ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† ØªØ±Ø¬Ù…Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§Ù„Ù‡ ÙØ´Ù† Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø²ÛŒØ±:
   - Headline: Û¸â€“Û±Û´ Ú©Ù„Ù…Ù‡ Ø¬Ø°Ø§Ø¨
   - Subheadline: Û± Ø¬Ù…Ù„Ù‡ ØªÚ©Ù…ÛŒÙ„ÛŒ
   - Lead: Û±â€“Û² Ø¬Ù…Ù„Ù‡ (Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ú†Ù‡ØŒ Ú©ÛŒØŒ Ú©Ø¬Ø§ØŒ Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³Øª)
   - Body: Û³â€“Ûµ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ú©ÙˆØªØ§Ù‡ Ùˆ Ø±ÙˆØ§Ù†
   - Industry Insight: Û²â€“Û´ Ø¬Ù…Ù„Ù‡ ØªØ­Ù„ÛŒÙ„ÛŒ (ØªØ£Ø«ÛŒØ± Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ù…Ø¯ØŒ Ø§Ø³ØªØ§ÛŒÙ„ Ø§ÛŒØ±Ø§Ù†ÛŒØŒ ÛŒØ§ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ)

Û³. Ø·ÙˆÙ„ Ú©Ù„: Û²ÛµÛ°â€“Û´ÛµÛ° Ú©Ù„Ù…Ù‡
Û´. Ù„Ø­Ù†: Ø­Ø±ÙÙ‡â€ŒØ§ÛŒØŒ Ú˜ÙˆØ±Ù†Ø§Ù„ÛŒØ³ØªÛŒØŒ Ø®Ù†Ø«ÛŒØŒ Ø¨Ø¯ÙˆÙ† ØªØ¨Ù„ÛŒØº
Ûµ. Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ…ÙˆØ¬ÛŒØŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø´ØªÚ¯ Ø¯Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ (Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯)

Ø¹Ù†ÙˆØ§Ù†: {title}
Ù…ØªÙ† Ø®Ø§Ù…: {content_raw[:1200]}
Ù„ÛŒÙ†Ú©: {link}
ØªØ§Ø±ÛŒØ®: {pub_date.strftime('%Y-%m-%d')}

Ø®Ø±ÙˆØ¬ÛŒ ÙÙ‚Ø· Ù…Ù‚Ø§Ù„Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§Ø´Ù‡ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÛŒ.
Ø§Ú¯Ø± ØºÛŒØ±Ù…Ø±ØªØ¨Ø· Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³ "ØºÛŒØ±Ù…Ø±ØªØ¨Ø·".
"""

    try:
        response = await client.chat.completions.create(
            model="gemini-2.5-flash",  # Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø² proØŒ ÙˆÙ„ÛŒ Ú©ÛŒÙÛŒØª ÙØ§Ø±Ø³ÛŒ Ù‡Ù†ÙˆØ² Ø¹Ø§Ù„ÛŒ
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1800,
            temperature=0.4
        )
        result = response.choices[0].message.content.strip()

        if "ØºÛŒØ±Ù…Ø±ØªØ¨Ø·" in result:
            return None

        return result
    except Exception as e:
        print(f"[ERROR] Ø®Ø·Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ù‡: {str(e)}")
        return None


def get_image_from_rss(entry):
    if 'enclosure' in entry and entry.enclosure.get('type', '').startswith('image/'):
        return entry.enclosure.href
    if 'media_content' in entry:
        for media in entry.media_content:
            if media.get('medium') == 'image' and media.get('url'):
                return media.get('url')
    return None


async def extract_image_from_page(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, timeout=8, headers=headers)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')
        og = soup.find('meta', property='og:image')
        if og and og.get('content'):
            return og['content']

        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src and len(src) > 15:
                if any(bad in src.lower() for bad in ['logo', 'icon', 'banner']):
                    continue
                return src
        return None
    except:
        return None


if __name__ == "__main__":
    asyncio.run(main())
