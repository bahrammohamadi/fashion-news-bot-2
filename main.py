# main_fashion_gapgpt_v9.py - ÙÛŒÚ©Ø³ rate limit + Û± Ù¾Ø³Øª + ØªØ±Ø¬Ù…Ù‡ ØªØ±Ú©ÛŒØ¨ÛŒ + fallback Ù‚ÙˆÛŒ

import os
import asyncio
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from telegram import Bot
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.exception import AppwriteException
from appwrite.query import Query
import random

# ====================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ======================
TELEGRAM_BOT_TOKEN = os.environ.get('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHANNEL_ID = os.environ.get('TELEGRAM_CHANNEL_ID')
GAPGPT_API_KEY = os.environ.get('GAPGPT_API_KEY')  # Ú©Ù„ÛŒØ¯ GapGPT/OpenRouter
APPWRITE_ENDPOINT = os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1')
APPWRITE_PROJECT_ID = os.environ.get('APPWRITE_PROJECT_ID')
APPWRITE_API_KEY = os.environ.get('APPWRITE_API_KEY')
APPWRITE_DATABASE_ID = os.environ.get('APPWRITE_DATABASE_ID')
COLLECTION_ID = 'history'

MAX_POSTS_PER_RUN = 1  # ÙÙ‚Ø· Û± Ù¾Ø³Øª Ø¯Ø± Ù‡Ø± Ø§Ø¬Ø±Ø§ (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² rate limit)
CHECK_DAYS = 4
MAX_RAW_TEXT_LENGTH = 1200
MAX_FINAL_TEXT_LENGTH = 420
HTTP_TIMEOUT = 10

# ====================== ÙÛŒØ¯Ù‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø¯ Ùˆ ÙØ´Ù† ======================
RSS_FEEDS = [
    "https://www.vogue.com/feed/rss",
    "https://wwd.com/feed/",
    "https://www.harpersbazaar.com/rss/fashion.xml",
    "https://fashionista.com/feed",
    "https://www.businessoffashion.com/feed/",
    "https://www.elle.com/rss/fashion.xml",
    "https://www.refinery29.com/rss.xml",
    "https://www.thecut.com/feed",
    "https://www.whowhatwear.com/rss",
    "https://www.instyle.com/rss",
    "https://www.marieclaire.com/rss/fashion/",
    "https://www.glamour.com/rss/fashion",
    "https://www.allure.com/rss",
    "https://nylon.com/feed",
    "https://www.highsnobiety.com/feed/",
    "https://hypebeast.com/feed",
    "https://www.ssense.com/en-us/editorial/rss",
    "https://www.dazeddigital.com/rss",
    "https://i-d.vice.com/en/rss",
    "https://www.papermag.com/rss",
]

# ====================== Ù†Ú©Ø§Øª Ø§Ø³ØªØ§ÛŒÙ„ ÙØ§Ø±Ø³ÛŒ (ØªØµØ§Ø¯ÙÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´Ù‡) ======================
STYLE_TIPS = [
    "Ø§ÛŒÙ† ØªØ±Ù†Ø¯ Ø±Ùˆ Ø¨Ø§ Ù…Ø§Ù†ØªÙˆ Ø¨Ù„Ù†Ø¯ Ùˆ Ø´Ø§Ù„ Ø³Ø§Ø¯Ù‡ ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯ ØªØ§ Ø§Ø³ØªØ§ÛŒÙ„ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø´ÛŒÚ©â€ŒØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.",
    "Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ø²Ù†Ø¯Ù‡ Ø§Ù…Ø³Ø§Ù„ Ø±Ùˆ Ø¨Ø§ Ø§Ú©Ø³Ø³ÙˆØ±ÛŒ Ø·Ù„Ø§ÛŒÛŒ ÛŒØ§ Ù†Ù‚Ø±Ù‡â€ŒØ§ÛŒ Ø³Øª Ú©Ù†ÛŒØ¯ ØªØ§ Ø¬Ù„ÙˆÙ‡ Ù…Ø¬Ù„Ø³ÛŒâ€ŒØªØ±ÛŒ Ø¨Ú¯ÛŒØ±Ù‡.",
    "Ù„Ø§ÛŒÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ú©Øª Ú©ÙˆØªØ§Ù‡ Ø±ÙˆÛŒ Ù…Ø§Ù†ØªÙˆ Ø¨Ù„Ù†Ø¯ØŒ ØªØ±Ù†Ø¯ Ø§Ø¯Ø§Ø±ÛŒ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ù…Ø³Ø§Ù„Ù‡.",
    "Ø´Ù„ÙˆØ§Ø±Ù‡Ø§ÛŒ Ø¨Ø§Ù„ÙˆÙ†ÛŒ Ø±Ùˆ Ø¨Ø§ Ù…Ø§Ù†ØªÙˆ Ø§ÙˆØ±Ø³Ø§ÛŒØ² Ùˆ Ú©ÙØ´ Ú©ØªØ§Ù†ÛŒ Ø³Øª Ú©Ù†ÛŒØ¯Ø› Ø±Ø§Ø­ØªÛŒ + Ù…Ø¯!",
    "Ù¾Ø§Ø±Ú†Ù‡â€ŒÙ‡Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ (Ù†Ø®ÛŒØŒ Ù„ÛŒÙ†Ù†) Ø±Ùˆ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø¯ÛŒØ¯Ø› Ù‡Ù… Ø®Ù†Ú© Ù‡Ø³ØªÙ† Ù‡Ù… Ø¨Ø§ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ÛŒ Ø§ÛŒØ±Ø§Ù† Ù‡Ù…Ø§Ù‡Ù†Ú¯.",
    "Ø±Ù†Ú¯ ÙÛŒØ±ÙˆØ²Ù‡â€ŒØ§ÛŒ Ø³Ø§Ù„ Û²Û°Û²Û¶ Ø±Ùˆ Ø¨Ø§ Ø¨Ú˜ ÛŒØ§ Ø®Ø§Ú©Ø³ØªØ±ÛŒ ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯Ø› Ù…ÛŒÙ†ÛŒÙ…Ø§Ù„ Ùˆ Ø¬Ø°Ø§Ø¨.",
    "Ú©ÛŒÙ Ú©ÙˆÚ†Ú© Ø±ÙˆÛŒ Ú©Ù…Ø±Ø¨Ù†Ø¯ (Bag-on-belt) Ø±Ùˆ Ø¨Ù‡ Ù…Ø§Ù†ØªÙˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯Ø› ØªØ±Ù†Ø¯ Ø®ÛŒØ§Ø¨Ø§Ù†ÛŒ Ø¯Ø§Øº!",
    "Ø±ÙˆØ³Ø±ÛŒ Ø³Ø§ØªÙ† Ø¨Ø±Ø§Ù‚ Ø¨Ø§ Ù…Ø§Ù†ØªÙˆ Ø³Ø§Ø¯Ù‡ Ùˆ Ø¬ÙˆØ§Ù‡Ø±Ø§Øª Ø­Ø¬ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ù…Ø§Ù†ÛŒ Ø¹Ø§Ù„ÛŒ Ù…ÛŒâ€ŒØ´Ù‡.",
    "Ø¬Ø²Ø¦ÛŒØ§Øª Ú©ÙˆÚ†Ú© Ù…Ø«Ù„ Ú©Ù…Ø±Ø¨Ù†Ø¯ Ø¨Ø§Ø±ÛŒÚ© ÛŒØ§ Ø¢Ø³ØªÛŒÙ† Ù¾Ùâ€ŒØ¯Ø§Ø±ØŒ Ø§Ø³ØªØ§ÛŒÙ„ Ø±Ùˆ Ø®ÛŒÙ„ÛŒ Ø®Ø§Øµ Ù…ÛŒâ€ŒÚ©Ù†Ù†.",
    "Ø§Ø³ØªØ§ÛŒÙ„ Ø¨ÙˆÙ‡Ùˆ Ø±Ùˆ Ø¨Ø§ Ù…Ø§Ù†ØªÙˆ Ø¨Ù„Ù†Ø¯ Ùˆ Ø´Ø§Ù„ Ø·Ø±Ø­â€ŒØ¯Ø§Ø± Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯Ø› Ø­Ø³ Ø¢Ø²Ø§Ø¯ÛŒ Ùˆ Ø²ÛŒØ¨Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ¯Ù‡."
]

# ====================== ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ GapGPT ======================
async def translate_and_format(client, title, raw_text):
    prompt = f"""
Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±: {title}
Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: {raw_text[:1200]}

Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù†ØŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø¬Ø°Ø§Ø¨ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„ Ù…Ø¯ ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†.
- ØªÛŒØªØ± Ø±Ø§ Ø¬Ø°Ø§Ø¨ Ùˆ Ú©ÙˆØªØ§Ù‡ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
- Ù…ØªÙ† Ø±Ø§ Û³-Û¶ Ø®Ø·ÛŒØŒ Ø´ÛŒÚ© Ùˆ Ø®Ù„Ø§ØµÙ‡ Ø¨Ù†ÙˆÛŒØ³
- ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
- Ø¨Ø¯ÙˆÙ† Ø¬Ù…Ù„Ù‡ Ø§Ø¶Ø§ÙÙ‡ØŒ ØªØ¨Ù„ÛŒØºØŒ Ø§ÛŒÙ…ÙˆØ¬ÛŒ ÛŒØ§ Ù„ÛŒÙ†Ú©
- Ø§Ú¯Ø± Ø®Ø¨Ø± Ø¨Ù‡ Ø§Ø³ØªØ§ÛŒÙ„ ÛŒØ§ ØªØ±Ù†Ø¯ Ù„Ø¨Ø§Ø³ Ù…Ø±Ø¨ÙˆØ· Ø¨ÙˆØ¯ØŒ ÛŒÚ© Ù†Ú©ØªÙ‡ Ú©ÙˆØªØ§Ù‡ Ø§Ø³ØªØ§ÛŒÙ„ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†

Ø®Ø±ÙˆØ¬ÛŒ ÙÙ‚Ø· Ù…ØªÙ† Ù¾Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§Ø´Ù‡ (ØªÛŒØªØ± + Ù…ØªÙ† + Ù†Ú©ØªÙ‡ Ø§Ø³ØªØ§ÛŒÙ„ Ø§Ø®ØªÛŒØ§Ø±ÛŒ).
"""

    try:
        resp = await client.chat.completions.create(
            model="gpt-4o-mini",  # Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹ Ùˆ Ø§Ø±Ø²Ø§Ù† GapGPT
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=600
        )
        final_text = resp.choices[0].message.content.strip()
        if not final_text or len(final_text) < 50:
            raise Exception("Ù¾Ø§Ø³Ø® Ù†Ø§Ù…Ø¹ØªØ¨Ø±")
        return final_text
    except Exception as e:
        print(f"[GAPGPT ERROR] {str(e)[:100]} - fallback")
        clean_fallback = clean_html(raw_text)
        if len(clean_fallback) > MAX_FINAL_TEXT_LENGTH:
            clean_fallback = clean_fallback[:MAX_FINAL_TEXT_LENGTH] + "..."
        tip = random.choice(STYLE_TIPS) if random.random() < 0.5 else ""
        fallback_text = f"**{title}**\n\n{clean_fallback}"
        if tip:
            fallback_text += f"\n\nğŸ’¡ Ù†Ú©ØªÙ‡ Ø§Ø³ØªØ§ÛŒÙ„: {tip}"
        return fallback_text

# ====================== ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ======================
def clean_html(html):
    soup = BeautifulSoup(html, 'html.parser')
    text = soup.get_text(separator=' ', strip=True)
    return ' '.join(text.split())

def get_image_from_rss(entry):
    if 'enclosure' in entry and entry.enclosure.get('type', '').startswith('image/'):
        return entry.enclosure.get('href')
    if 'media_content' in entry:
        for media in entry.media_content:
            if media.get('medium') == 'image' and media.get('url'):
                return media.get('url')
    return None

async def extract_og_image(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, timeout=8, headers=headers)
        if response.status_code != 200:
            return None
        soup = BeautifulSoup(response.text, 'html.parser')
        og = soup.find('meta', property='og:image')
        if og and og.get('content'):
            return og['content']
        return None
    except Exception as e:
        print(f"[OG IMAGE ERROR] {str(e)[:100]}")
        return None

# ====================== ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ======================
async def main(event=None, context=None):
    print("[INFO] Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§")

    if not all([TELEGRAM_BOT_TOKEN, TELEGRAM_CHANNEL_ID, GAPGPT_API_KEY, APPWRITE_PROJECT_ID, APPWRITE_API_KEY, APPWRITE_DATABASE_ID]):
        print("[ERROR] Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ù†Ø§Ù‚Øµ")
        return {"status": "error"}

    bot = Bot(token=TELEGRAM_BOT_TOKEN)

    gapgpt_client = AsyncOpenAI(
        api_key=GAPGPT_API_KEY,
        base_url="https://api.gapgpt.app/v1"  # GapGPT endpoint
    )

    aw_client = Client()
    aw_client.set_endpoint(APPWRITE_ENDPOINT)
    aw_client.set_project(APPWRITE_PROJECT_ID)
    aw_client.set_key(APPWRITE_API_KEY)
    databases = Databases(aw_client)

    now = datetime.now(timezone.utc)
    time_threshold = now - timedelta(days=CHECK_DAYS)

    posted_count = 0

    for feed_url in RSS_FEEDS:
        if posted_count >= MAX_POSTS_PER_RUN:
            break

        try:
            feed = feedparser.parse(feed_url)
            if not feed.entries:
                continue

            for entry in feed.entries:
                if posted_count >= MAX_POSTS_PER_RUN:
                    break

                published = entry.get('published_parsed') or entry.get('updated_parsed')
                if not published:
                    continue
                pub_date = datetime(*published[:6], tzinfo=timezone.utc)
                if pub_date < time_threshold:
                    continue

                title = entry.title.strip()
                link = entry.link.strip()
                raw_content = (entry.get('summary') or entry.get('description') or '')[:MAX_RAW_TEXT_LENGTH]

                soup = BeautifulSoup(raw_content, 'html.parser')
                clean_text = soup.get_text(separator=' ').strip()

                # Ú†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ
                try:
                    existing = databases.list_documents(
                        database_id=APPWRITE_DATABASE_ID,
                        collection_id=COLLECTION_ID,
                        queries=[Query.equal("link", link)]
                    )
                    if existing['total'] > 0:
                        continue
                except Exception as e:
                    print(f"[DB WARN] {e}")

                # ØªØ±Ø¬Ù…Ù‡ Ùˆ ÙØ±Ù…Øª Ø¨Ø§ GapGPT
                final_text = await translate_and_format(gapgpt_client, title, clean_text)

                image_url = get_image_from_rss(entry)
                if not image_url:
                    image_url = await extract_og_image(link)

                try:
                    if image_url:
                        await bot.send_photo(
                            chat_id=TELEGRAM_CHANNEL_ID,
                            photo=image_url,
                            caption=final_text,
                            parse_mode='HTML',
                            disable_notification=True
                        )
                    else:
                        await bot.send_message(
                            chat_id=TELEGRAM_CHANNEL_ID,
                            text=final_text,
                            parse_mode='HTML',
                            disable_notification=True
                        )

                    posted_count += 1
                    print(f"[SUCCESS] Ù¾Ø³Øª Ø´Ø¯: {title[:60]}")

                    try:
                        databases.create_document(
                            database_id=APPWRITE_DATABASE_ID,
                            collection_id=COLLECTION_ID,
                            document_id='unique()',
                            data={
                                'link': link,
                                'title': title[:250],
                                'published_at': now.isoformat(),
                                'feed_url': feed_url
                            }
                        )
                    except Exception as save_err:
                        print(f"[DB SAVE WARN] {save_err}")

                except Exception as send_err:
                    print(f"[SEND ERROR] {send_err}")

        except Exception as feed_err:
            print(f"[FEED ERROR] {feed_url}: {feed_err}")

    print(f"[INFO] Ù¾Ø§ÛŒØ§Ù† Ø§Ø¬Ø±Ø§ - Ù¾Ø³Øª Ø´Ø¯Ù‡: {posted_count}")
    return {"status": "ok", "posted": posted_count}


if __name__ == "__main__":
    asyncio.run(main())
