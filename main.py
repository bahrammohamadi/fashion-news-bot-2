import feedparser
import requests
import os
import time
from bs4 import BeautifulSoup
from googletrans import Translator

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø·Ø¨Ù‚ ØªØµØ§ÙˆÛŒØ± Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§
PROJECT_ID = "699039d4000e86c2f95e"
DATABASE_ID = "6990a1310017aa6c5c0d"
COLLECTION_ID = "history"

def is_duplicate(link):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø³Øª ØªÚ©Ø±Ø§Ø±ÛŒ"""
    url = f"https://cloud.appwrite.io/v1/databases/{DATABASE_ID}/collections/{COLLECTION_ID}/documents"
    headers = {"X-Appwrite-Project": PROJECT_ID}
    # Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† ÙˆØ¬ÙˆØ¯ Ù„ÛŒÙ†Ú©
    params = {"queries[]": f'equal("link", ["{link}"])'}
    try:
        res = requests.get(url, headers=headers, params=params, timeout=10)
        return res.json().get('total', 0) > 0
    except:
        return False

def save_to_db(link, title):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ù¾Ø±Ø§ÛŒØª"""
    url = f"https://cloud.appwrite.io/v1/databases/{DATABASE_ID}/collections/{COLLECTION_ID}/documents"
    headers = {
        "X-Appwrite-Project": PROJECT_ID,
        "Content-Type": "application/json"
    }
    payload = {
        "documentId": "unique()",
        "data": {
            "link": link,
            "title": title[:255],
            "date": str(time.strftime("%Y-%m-%d %H:%M"))
        }
    }
    try:
        requests.post(url, headers=headers, json=payload, timeout=10)
    except:
        pass

def get_image(text):
    """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ú©Ø³ Ù…Ø±ØªØ¨Ø·"""
    try:
        url = f"https://www.google.com/search?q={text[:30]}+fashion+trend&tbm=isch"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        return soup.find_all("img")[2]['src']
    except:
        return None

def main(context):
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    channel_id = os.getenv('TELEGRAM_CHANNEL_ID')
    translator = Translator()

    # Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ
    feeds = ["https://www.vogue.com/feed/rss", "https://wwd.com/feed/"]
    
    posted_count = 0
    for f_url in feeds:
        if posted_count >= 2: break
        
        feed = feedparser.parse(f_url)
        for entry in feed.entries[:3]:
            if posted_count >= 2: break
            
            link = entry.link
            if is_duplicate(link): continue

            try:
                # ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
                fa_title = translator.translate(entry.title, dest='fa').text
                summary = entry.get('summary', '')[:200]
                fa_summary = translator.translate(summary, dest='fa').text if summary else ""

                caption = (
                    f"ğŸ‘— <b>{fa_title}</b>\n\n"
                    f"ğŸ’¡ {fa_summary}...\n\n"
                    f"ğŸ”— <a href='{link}'>Ø§Ø¯Ø§Ù…Ù‡ Ø®Ø¨Ø±</a>\n\n"
                    f"ğŸ· #Ù…Ø¯ #ÙØ´Ù† #Ø§Ø³ØªØ§ÛŒÙ„"
                )

                img = get_image(entry.title)

                # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
                t_url = f"https://api.telegram.org/bot{bot_token}/sendPhoto"
                res = requests.post(t_url, data={
                    'chat_id': channel_id,
                    'photo': img,
                    'caption': caption,
                    'parse_mode': 'HTML'
                })

                if res.status_code == 200:
                    save_to_db(link, fa_title)
                    posted_count += 1
                    time.sleep(5)
            except:
                continue

    return context.res.json({"status": "success", "posted": posted_count})
