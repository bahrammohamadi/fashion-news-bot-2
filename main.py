import google.generativeai as genai
import feedparser
import requests
import os
import time
import random
from bs4 import BeautifulSoup

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø·Ø¨Ù‚ Ø¹Ú©Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙØ±Ø³ØªØ§Ø¯ÛŒ)
PROJECT_ID = "699039d4000e86c2f95e"
DATABASE_ID = "6990a1310017aa6c5c0d"
COLLECTION_ID = "history"
GEMINI_KEY = "AIzaSyCHs8e_s6FryC1_HXgyf3HjJwn5SBx_llI"

def ask_gemini(text):
    try:
        genai.configure(api_key=GEMINI_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = (
            f"ØªÙˆ ÛŒÚ© Ø§Ø³ØªØ§ÛŒÙ„ÛŒØ³Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ù‡Ø³ØªÛŒ. Ø§ÛŒÙ† Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¬Ø°Ø§Ø¨ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… Ø®Ù„Ø§ØµÙ‡ Ú©Ù†. "
            f"Ù†Ú©Ø§Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Øª Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø§ Ù¾ÙˆØ´Ø´ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ù…Ø§Ù†Ù†Ø¯ Ù…Ø§Ù†ØªÙˆ Ùˆ Ø´Ø§Ù„) Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†. "
            f"Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ùˆ Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„ @irfashionnews Ø±Ø§ Ø¯Ø± Ù…ØªÙ† Ù†ÛŒØ§Ø± (Ø®ÙˆØ¯Ù… Ø¢Ø®Ø±Ø´ Ù…ÛŒâ€ŒØ°Ø§Ø±Ù…):\n\n{text}"
        )
        response = model.generate_content(prompt)
        return response.text
    except:
        return None

def is_duplicate(link):
    url = f"https://cloud.appwrite.io/v1/databases/{DATABASE_ID}/collections/{COLLECTION_ID}/documents"
    headers = {"X-Appwrite-Project": PROJECT_ID}
    params = {"queries[]": f'equal("link", ["{link}"])'}
    try:
        res = requests.get(url, headers=headers, params=params, timeout=10)
        return res.json().get('total', 0) > 0
    except:
        return False

def save_to_db(link, title):
    url = f"https://cloud.appwrite.io/v1/databases/{DATABASE_ID}/collections/{COLLECTION_ID}/documents"
    headers = {"X-Appwrite-Project": PROJECT_ID, "Content-Type": "application/json"}
    payload = {"documentId": "unique()", "data": {"link": link, "title": title[:250], "date": str(time.ctime())}}
    requests.post(url, headers=headers, json=payload, timeout=10)

def main(context):
    # Ø¨Ø®Ø´ ØªØ§Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§ÙˆÙ„ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù… ØªØ§ Ø³Ø±ÛŒØ¹ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    channel_id = os.getenv('TELEGRAM_CHANNEL_ID')

    feeds = [
        "https://www.vogue.com/feed/rss", 
        "https://wwd.com/feed/", 
        "https://shikpoushan.com/feed/",
        "https://modopia.com/feed/"
    ]
    
    random.shuffle(feeds)
    posted_count = 0

    for f_url in feeds:
        if posted_count >= 2: break
        feed = feedparser.parse(f_url)
        for entry in feed.entries[:3]:
            if posted_count >= 2: break
            if is_duplicate(entry.link): continue

            try:
                ai_caption = ask_gemini(f"{entry.title}\n{entry.get('summary', '')[:500]}")
                if not ai_caption: continue

                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ú©Ø³ Ù…Ø±ØªØ¨Ø·
                img_url = f"https://www.google.com/search?q={entry.title[:30]}+fashion&tbm=isch"
                res_img = requests.get(img_url, headers={"User-Agent": "Mozilla/5.0"})
                soup = BeautifulSoup(res_img.text, 'html.parser')
                img = soup.find_all("img")[2]['src']

                final_text = f"{ai_caption}\n\nâœ¨ @irfashionnews\nğŸ· #Ù…Ø¯ #Ø§Ø³ØªØ§ÛŒÙ„ #ÙØ´Ù†"

                requests.post(f"https://api.telegram.org/bot{bot_token}/sendPhoto", data={
                    'chat_id': channel_id, 'photo': img, 'caption': final_text, 'parse_mode': 'HTML'
                })

                save_to_db(entry.link, entry.title)
                posted_count += 1
                time.sleep(5)
            except:
                continue

    return context.res.json({"status": "success", "posted": posted_count})
