import google.generativeai as genai
import feedparser
import requests
import os
import time
import random
from bs4 import BeautifulSoup

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ Ø§Ù¾â€ŒØ±Ø§ÛŒØª (Ø·Ø¨Ù‚ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø´Ù…Ø§)
PROJECT_ID = "699039d4000e86c2f95e"
DATABASE_ID = "6990a1310017aa6c5c0d"
COLLECTION_ID = "history"
GEMINI_KEY = "AIzaSyCHs8e_s6FryC1_HXgyf3HjJwn5SBx_llI"

def ask_gemini(text):
    """ØªØ­Ù„ÛŒÙ„ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ù†Ú¯Ø§Ù‡ Ø¨Ù‡ Ø§Ø³ØªØ§ÛŒÙ„ Ùˆ ÙØ±Ù‡Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ"""
    try:
        genai.configure(api_key=GEMINI_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        prompt = (
            f"ØªÙˆ ÛŒÚ© Ø³Ø±Ø¯Ø¨ÛŒØ± Ù…Ø¬Ù„Ù‡ Ù…Ø¯ Ùˆ Ø§Ø³ØªØ§ÛŒÙ„ÛŒØ³Øª Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ù‡Ø³ØªÛŒ. "
            f"Ø§ÛŒÙ† Ù…ØªÙ† Ø®Ø¨Ø±ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø®ÛŒÙ„ÛŒ Ø¬Ø°Ø§Ø¨ØŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… Ø®Ù„Ø§ØµÙ‡ Ú©Ù†. "
            f"Ø­ØªÙ…Ø§Ù‹ Ù†Ú©Ø§Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Øª Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† Ø§Ø³ØªØ§ÛŒÙ„ Ø¨Ø§ Ù¾ÙˆØ´Ø´ Ø´ÛŒÚ© Ø§ÛŒØ±Ø§Ù†ÛŒ (Ù…Ø§Ù†Ù†Ø¯ Ù…Ø§Ù†ØªÙˆØŒ Ø´Ø§Ù„ ÛŒØ§ Ù„Ø§ÛŒÙ‡â€ŒØ¨Ù†Ø¯ÛŒ) Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†. "
            f"Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ùˆ Ù„Ø­Ù†Øª Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø¬Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§Ø´Ø¯:\n\n{text}"
        )
        
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"Gemini Error: {e}")
        return None

def is_duplicate(link):
    """Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨ÙˆØ¯Ù† Ø®Ø¨Ø± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    url = f"https://cloud.appwrite.io/v1/databases/{DATABASE_ID}/collections/{COLLECTION_ID}/documents"
    headers = {"X-Appwrite-Project": PROJECT_ID}
    params = {"queries[]": f'equal("link", ["{link}"])'}
    try:
        res = requests.get(url, headers=headers, params=params, timeout=10)
        return res.json().get('total', 0) > 0
    except:
        return False

def save_to_db(link, title):
    """Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±"""
    url = f"https://cloud.appwrite.io/v1/databases/{DATABASE_ID}/collections/{COLLECTION_ID}/documents"
    headers = {"X-Appwrite-Project": PROJECT_ID, "Content-Type": "application/json"}
    payload = {
        "documentId": "unique()",
        "data": {
            "link": link,
            "title": title[:250],
            "date": str(time.ctime())
        }
    }
    requests.post(url, headers=headers, json=payload, timeout=10)

def main(context):
    # Û±. ØªØ§Ø®ÛŒØ± ØªØµØ§Ø¯ÙÛŒ Ø¨ÛŒÙ† Û± ØªØ§ Û±Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙØªØ§Ø± Ø§Ù†Ø³Ø§Ù†ÛŒ
    time.sleep(random.randint(60, 900))
    
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    channel_id = os.getenv('TELEGRAM_CHANNEL_ID')

    # Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ùˆ Ø¢Ù…ÙˆØ²Ø´ÛŒ
    feeds = [
        "https://www.vogue.com/feed/rss", 
        "https://wwd.com/feed/", 
        "https://fashionista.com/.rss/full/",
        "https://www.elle.com/rss/all.xml",
        "https://shikpoushan.com/feed/",
        "https://chibepoosham.com/feed/",
        "https://komodomode.com/mag/feed/",
        "https://modopia.com/feed/",
        "https://www.whowhatwear.com/rss"
    ]
    
    random.shuffle(feeds) 
    posted_count = 0

    for f_url in feeds:
        if posted_count >= 2: break 
        
        feed = feedparser.parse(f_url)
        for entry in feed.entries[:3]:
            if posted_count >= 2: break
            
            link = entry.link
            if is_duplicate(link):
                continue

            try:
                # Û². Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Gemini
                content_to_analyze = f"Title: {entry.title}\nSummary: {entry.get('summary', '')[:500]}"
                ai_caption = ask_gemini(content_to_analyze)
                
                if not ai_caption: continue

                # Û³. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØµÙˆÛŒØ± Ù…Ø±ØªØ¨Ø·
                search_query = entry.title.split('|')[0]
                img_url = f"https://www.google.com/search?q={search_query[:40]}+fashion+style&tbm=isch"
                res_img = requests.get(img_url, headers={"User-Agent": "Mozilla/5.0"})
                soup = BeautifulSoup(res_img.text, 'html.parser')
                img = soup.find_all("img")[2]['src']

                # Û´. Ø³Ø§Ø®Øª Ú©Ù¾Ø´Ù† Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„ Ø´Ù…Ø§
                final_text = (
                    f"{ai_caption}\n\n"
                    f"âœ¨ @irfashionnews\n"
                    f"ğŸ· #Ù…Ø¯ #Ø§Ø³ØªØ§ÛŒÙ„ #Ø¢Ù…ÙˆØ²Ø´ #ÙØ´Ù† #ØªÛŒÙ¾_Ø§ÛŒØ±Ø§Ù†ÛŒ"
                )

                # Ûµ. Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
                requests.post(f"https://api.telegram.org/bot{bot_token}/sendPhoto", data={
                    'chat_id': channel_id,
                    'photo': img,
                    'caption': final_text,
                    'parse_mode': 'HTML'
                })

                save_to_db(link, entry.title)
                posted_count += 1
                time.sleep(15) 
            except:
                continue

    return context.res.json({"status": "success", "posted": posted_count})
